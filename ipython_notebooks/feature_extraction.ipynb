{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "# This file is (was?) in /l/rkarhila/speecon_wsj_phoneme_dnn/data_preprocessing\n",
    "\n",
    "#\n",
    "#  1. Divide each data file into single phoneme chunks based on aliged labels\n",
    "#\n",
    "#  2. Run the chunks through feature extraction shell script\n",
    "#\n",
    "#  3. Store the features and their associated phoneme information in arrays\n",
    "#\n",
    "#  4. Pickle for future normalisation (with other corpora) \n",
    "#\n",
    "\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import re\n",
    "import math \n",
    "import struct\n",
    "import time\n",
    "import sys\n",
    "import struct\n",
    "import random\n",
    "\n",
    "#\n",
    "# Use some funky structure from tensor flow to store 3d-matrices of variable length more compactly.\n",
    "#\n",
    "import tensorflow as tf\n",
    "\n",
    "#\n",
    "# A function that will be useful:\n",
    "#\n",
    "\n",
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)        \n",
    "    except OSError as exc:  # Python >2.5\n",
    "        print (\"dir %s exists\" % path)\n",
    "\n",
    "#\n",
    "# Some more output?\n",
    "#\n",
    "debug=True\n",
    "global debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessing_scripts = [{'script': '../feature_extraction_scripts/preprocess_pfstar.sh', 'name' : 'clean', 'parameters': [[0,0], [0,0]] },\n",
    "                         {'script': '../feature_extraction_scripts/preprocess_pfstar.sh', 'name' : 'clean', 'parameters': [[0,0], [0,0]] },\n",
    "                         {'script': '../feature_extraction_scripts/preprocess_pfstar_and_overdrive.sh', 'name' : 'overdrive', 'parameters': [[1,10], [-20,0]] },\n",
    "                         {'script': '../feature_extraction_scripts/preprocess_pfstar_and_overdrive.sh', 'name' : 'underdrive', 'parameters': [[-40,-20], [0,0]] },\n",
    "                         {'script': '../feature_extraction_scripts/preprocess_pfstar_and_add_babble.sh', 'name' : 'babbled', 'parameters': [[-40,-10],[-20,0]] },\n",
    "                         {'script': '../feature_extraction_scripts/preprocess_pfstar_and_add_humming.sh', 'name' : 'volvo', 'parameters': [[-30,-10],[-20,0]] } ]\n",
    "\n",
    "feature_extraction_script = '../feature_extraction_scripts/extract_with_start_end.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using tmp dir /dev/shm/siak-feat-extract-python-1480952158.3560872\n"
     ]
    }
   ],
   "source": [
    "samples_per_class_per_speaker = 20\n",
    "fs = 16000\n",
    "\n",
    "\n",
    "vowels = ['a','A','å','Å','ä','Ä','e','E','f','i','I','o','O','ö','u','U']\n",
    "\n",
    "nonvow = ['b','C','d','D','g','H','j','J','k','l','m','n','N','p','P','Q','r','R','s','S','t','T','v','w','W','Y','z','Z']\n",
    "\n",
    "combinations = []\n",
    "\n",
    "\n",
    "used_classes = vowels+nonvow+combinations\n",
    "classes_name = \"mc_en_uk_all\"\n",
    "\n",
    "\n",
    "#\n",
    "# Settings for feature extraction:\n",
    "#\n",
    "\n",
    "datatypelength = 2 # 16 bits = 2 bytes, no?\n",
    "\n",
    "frame_length = 400\n",
    "frame_step = 128\n",
    "\n",
    "frame_leftovers = frame_length-frame_step\n",
    "\n",
    "padding_array = bytearray()\n",
    "\n",
    "progress_length = 80\n",
    "\n",
    "max_num_samples=8000 # 0.5 should be enough for any reasonable phoneme, right?\n",
    "\n",
    "max_num_classes = 10000\n",
    "feature_dimension=30\n",
    "\n",
    "max_num_frames=40\n",
    "max_num_monoclasses = 200\n",
    "max_num_monoclasses = 9\n",
    "\n",
    "\n",
    "#max_num_samples=100160\n",
    "assigned_num_samples=100\n",
    "\n",
    "\n",
    "\n",
    "# tmp directory for feature extraction.\n",
    "# This should reside in memory (tempfs or whatever it's called, often under /dev/shm/)\n",
    "\n",
    "tmp_dir=\"/dev/shm/siak-feat-extract-python-\"+str(time.time())\n",
    "try:\n",
    "    os.makedirs(tmp_dir)\n",
    "except OSError as exc:  # Python >2.5\n",
    "    if exc.errno == errno.EEXIST and os.path.isdir(tmp_dir):\n",
    "        pass\n",
    "    else:\n",
    "        raise   \n",
    "\n",
    "\n",
    "print ('using tmp dir %s' % tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset definitions ##\n",
    "*In a very awkward manner, we'll specify some local files that contain list of audio and transcription files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "#   Data collection defitinions - train, dev and eval sets:\n",
    "#\n",
    "\n",
    "\n",
    "corpus = \"en_uk_kids_align_from_clean\"\n",
    "pickle_dir='../features/work_in_progress/'+corpus+'/pickles'\n",
    "statistics_dir = '../features/work_in_progress/'+corpus+'/statistics/'\n",
    "\n",
    "collections = [                                                                                                          \n",
    "    { 'name' : 'train-0',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.train.00',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 878 },\n",
    "    { 'name' : 'train-1',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.train.01',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 1083 },\n",
    "    { 'name' : 'train-2',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.train.02',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 946 },\n",
    "    { 'name' : 'train-3',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.train.03',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 870 },\n",
    "    { 'name' : 'train-4',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.train.04',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 651 },\n",
    "    { 'name' : 'train-5',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.train.05',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 785},\n",
    "    { 'name' : 'train-6',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.train.06',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 699 },\n",
    "    { 'name' : 'train-7',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.train.07',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 699 },\n",
    "    { 'name' : 'test-0',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.test.00',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 852 },\n",
    "    { 'name' : 'test-1',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.test.01',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 752 },\n",
    "    { 'name' : 'test-2',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.test.02',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 594 },\n",
    "    { 'name' : 'test-3',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.test.03',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 758 },\n",
    "    { 'name' : 'test-4',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.test.04',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 734 },\n",
    "    { 'name' : 'test-5',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.test.05',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 393},\n",
    "    { 'name' : 'eval-0',\n",
    "      'recipe' : '/l/rkarhila/speecon_wsj_phoneme_dnn/kids_en_uk/leave_one_out_recipes/recipe.speakers.eval.00',\n",
    "      'condition' : 'clean',\n",
    "      'numlines': 837 }\n",
    "]\n",
    "\n",
    "\n",
    "featdim1 = -1;\n",
    "featdim2 = -1;\n",
    "\n",
    "means_set = False\n",
    "means = -1;\n",
    "stds = -1;\n",
    "new_pickle_dir = \"-1\"\n",
    "\n",
    "classes = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions ##\n",
    "*Label processing etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_label( labelfile ):\n",
    "    global debug\n",
    "    \n",
    "    if not os.path.isfile(labelfile):\n",
    "        print (\"Can't find labelfile %s\" % labelfile)\n",
    "        return False\n",
    "    \n",
    "    with io.open(labelfile ,'r',encoding='iso-8859-15') as f:\n",
    "\n",
    "        new_align = []\n",
    "\n",
    "        current_start = 0\n",
    "        current_end = 0\n",
    "        current_model = False\n",
    "        current_premodel = False\n",
    "        current_postmodel = False\n",
    "\n",
    "        skip = False\n",
    "\n",
    "        phonect = 0\n",
    "        statect = 0\n",
    "\n",
    "        lcounter = 0\n",
    "\n",
    "        # For printing the phoneme sequences into a log:\n",
    "        skipmark=False\n",
    "\n",
    "        startmark=-1\n",
    "        endmark = -1\n",
    "\n",
    "        discard = False\n",
    "\n",
    "        phone={}\n",
    "\n",
    "        for l in  f.readlines():\n",
    "\n",
    "            # If we have a short pause model:\n",
    "            #if '+' not in l:\n",
    "            #    no_skipping = True\n",
    "            #    skipmark = True\n",
    "\n",
    "            # We'll process the label line by line with a two-phone delay:\n",
    "\n",
    "            if '+' in l and not discard:\n",
    "                #print \"Looking at %s\"%(l)\n",
    "                [start, \n",
    "                 end, \n",
    "                 premodel, \n",
    "                 model, \n",
    "                 postmodel, \n",
    "                 state] = re.split(r'[ .+-]', l.strip() ) #, l.encode('utf-8').strip() )\n",
    "\n",
    "                if state=='0':\n",
    "\n",
    "                    phone = {'start':start, \n",
    "                             'premodel':premodel, \n",
    "                             'model': model,\n",
    "                             'postmodel':postmodel,\n",
    "                             'state':state,\n",
    "                             'triphone': \"%s-%s+%s\" % (premodel, model, postmodel) }\n",
    "\n",
    "                if state=='2':\n",
    "                    phone['end'] = end\n",
    "\n",
    "                    if (phone['model'] != '__'):\n",
    "\n",
    "                        if (int(phone['end'])-int(phone['start']))/frame_step == 3:\n",
    "                            #discard_counter+=1\n",
    "                            #print \"Discarding %i/%i: %s: (Too short! Discards: %0.2f%s)\" % (recipefilecounter, collection['numlines'], labelfile, 100.0*discard_counter/collection['numlines'],\"%\" )\n",
    "\n",
    "                            discard = True\n",
    "\n",
    "                        #elif (int(phone['end'])-int(phone['start']))/frame_step > 40 and '_' not in phone['triphone']:\n",
    "                        #    #print \"Discarding %i/%i: %s (Too Long! Discards: %0.2f%s)\" % (recipefilecounter, collection['numlines'], labelfile, 100.0*discard_counter/collection['numlines'],\"%\" )\n",
    "                        #    discard_counter+=1\n",
    "                        #    discard = True\n",
    "\n",
    "                        #if debug:\n",
    "                        #    print (\"saving %s-%s+%s \" %  (phone['premodel'], phone['model'],phone['postmodel']))\n",
    "                        else:\n",
    "                            new_align.append({'pre' : phone['premodel'],\n",
    "                                          'model' : phone['model'],\n",
    "                                          'post' : phone['postmodel'],\n",
    "                                          'start' : phone['start'],\n",
    "                                          'end' : phone['end'],\n",
    "                                          'triphone' : phone['triphone'],\n",
    "                                          'sortable': \"%s--%s++%s\" % (phone['model'] , phone['premodel'], phone['postmodel'])\n",
    "                                      })\n",
    "\n",
    "\n",
    "    return new_align\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labelstring( new_align ):\n",
    "    labelstring = ''\n",
    "    for phone in new_align:\n",
    "        labelstring += '.'+phone['model']\n",
    "    return labelstring    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chop_features( audiodata, feature_array, new_align ):\n",
    "    global debug\n",
    "    \n",
    "    triphones = []\n",
    "\n",
    "    startmark = int(new_align[0]['start'])\n",
    "    endmark= int(new_align[-1]['end'])\n",
    "    \n",
    "    for l in new_align:                \n",
    "\n",
    "        lkey = l['sortable']\n",
    "        mkey = l['model']\n",
    "        tp = l['triphone']\n",
    "\n",
    "        l_start = (int(l['start'])-startmark)/frame_step\n",
    "        l_end =  (int(l['end'])-startmark)/frame_step\n",
    "        l_length = l_end - l_start\n",
    "\n",
    "        if (l_length == 3):\n",
    "            tooshortcount+=1\n",
    "            continue\n",
    "\n",
    "        # For debugging, let's write this stuff to disk:\n",
    "        if mkey not in quality_control_audio_files.keys():\n",
    "            qual_file = os.path.join(quality_control_wavdir,  mkey+\".wav\")\n",
    "            quality_control_audio_files[mkey] = open( qual_file , 'wb')\n",
    "\n",
    "        win_i=0\n",
    "        win_len=128\n",
    "        max_val=32000\n",
    "        norm=20000.0/max(abs(audiodata[int(l['start']):int(l['end'])]))\n",
    "        #print norm\n",
    "\n",
    "        for val in audiodata[int(l['start']):int(l['start'])+win_len]:\n",
    "            (quality_control_audio_files[mkey]).write( \n",
    "                    struct.pack( 'h', int( min( max_val,norm * val * win_i / win_len ) ) ) )\n",
    "            win_i+=1\n",
    "\n",
    "        for val in audiodata[int(l['start'])+win_len:int(l['end'])-win_len]:\n",
    "            (quality_control_audio_files[mkey]).write(\n",
    "                    struct.pack( 'h', int(min(max_val,norm * val ) ) ) ) \n",
    "\n",
    "        for val in audiodata[int(l['end'])-win_len:int(l['end'])]:\n",
    "            (quality_control_audio_files[mkey]).write(\n",
    "                    struct.pack( 'h', int(min(max_val,norm * val * win_i / win_len ) ) ) ) \n",
    "            win_i-=1\n",
    "\n",
    "\n",
    "        if (feature_array.shape[0] < l_end):\n",
    "            print (\"Not enough features: %i < %i\" % (feature_array.shape[0], l_end))\n",
    "            continue\n",
    "\n",
    "        statistics_handle.write(\"%i\\t%s\\n\" % (l_length, tp))\n",
    "\n",
    "        if debug:\n",
    "            print (\"---------------------------\")\n",
    "            print (\"Array stats: start %i -> %i length ?? -> %i end %i -> %i\" % (\n",
    "                        int(l['start'])-startmark, \n",
    "                        l_start, \n",
    "                        l_length, \n",
    "                        int(l['end'])-startmark, \n",
    "                        l_end ))\n",
    "            print (\"      phone data size: %i x %i\" % (feature_array[l_start:l_end, :]).shape)\n",
    "            print (\"Data size: %i x %i\" % feature_array.shape)\n",
    "\n",
    "        triphonedata.append ({ 'data': feature_array[l_start:l_start+max_num_frames, :],\n",
    "                                'counter': 0,\n",
    "                                'mono' :l['model'],\n",
    "                                'triphone' : l['triphone'],\n",
    "                                'sorting' : l['sortable'] })\n",
    "    return triphonedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features( audiofile, new_align):\n",
    "    global debug\n",
    "    \n",
    "    audiodata = np.fromfile( audiofile, 'int16', -1)\n",
    "\n",
    "    startmark = int(new_align[0]['start'])\n",
    "    endmark= int(new_align[-1]['end'])\n",
    "\n",
    "    if debug:\n",
    "        print (\"start feature extraction at %s (%f s) and end at %s (%f s) ==> %i frames\"  % (\n",
    "                startmark, \n",
    "                (float(startmark)/16000), \n",
    "                endmark, (float(endmark)/16000), \n",
    "                (endmark-startmark)/frame_step) )\n",
    "\n",
    "    # Communication from: \n",
    "    # http://stackoverflow.com/questions/163542/python-how-do-i-pass-a-string-into-subprocess-popen-using-the-stdin-argument\n",
    "\n",
    "    tmp_input=os.path.join(tmp_dir,str(tmpfilecounter)+\"_in\")\n",
    "    tmp_output=os.path.join(tmp_dir,str(tmpfilecounter)+\"_out\")\n",
    "\n",
    "    audiodata.tofile(tmp_input, \"\")\n",
    "\n",
    "    process_progress = Popen([\n",
    "            feature_extraction_script, \n",
    "            tmp_input, \n",
    "            tmp_output, \n",
    "            str(startmark), \n",
    "            str(endmark+frame_leftovers) ], \n",
    "                             stdout=PIPE, stdin=PIPE, stderr=STDOUT).communicate()\n",
    "\n",
    "    feature_list = np.fromfile(tmp_output, dtype='float32', count=-1)\n",
    "    feature_array = feature_list.reshape([-1,feature_dimension])\n",
    "\n",
    "    f_end =  (int(new_align[-1]['end'])-startmark)/frame_step\n",
    "\n",
    "    if debug:\n",
    "        print (\"Utterance data size: %i x %i\" % (feature_array).shape)\n",
    "\n",
    "    if (feature_array.shape[0] < f_end):\n",
    "            print (\"Not enough features for file %s: %i < %i\" % ...\n",
    "                   (audiofile, feature_array.shape[0], f_end))\n",
    "            print (\"panic save to /tmp/this_is_not_good\")\n",
    "            np.savetxt('/tmp/this_is_not_good', feature_array, delimiter='\\t')\n",
    "            raise ValueError(\"Not enough features for file %s: %i < %i\" % (\n",
    "                    audiofile, \n",
    "                    feature_array.shape[0], \n",
    "                    f_end) )\n",
    "    else:\n",
    "\n",
    "        chop_features( audiodata, feature_array, new_align )\n",
    "\n",
    "        triphonedata = chop_features( audiodata, feature_array, new_align )\n",
    "    \n",
    "    os.remove(tmp_input)\n",
    "    os.remove(tmp_output)\n",
    "    \n",
    "    return triphonedata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction (spectral/vocoder parameters) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n",
      "dir ../features/work_in_progress/en_uk_kids_align_from_clean/pickles/control-wav/clean-train-0-classes_mc_en_uk_all exists\n",
      "Item 1/878\n",
      "start feature extraction at 1792 (0.112000 s) and end at 30720 (1.920000 s) ==> 226 frames\n",
      "Utterance data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 0 -> 0 length ?? -> 32 end 4096 -> 32\n",
      "      phone data size: 32 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 4096 -> 32 length ?? -> 36 end 8704 -> 68\n",
      "      phone data size: 36 x 30\n",
      "Data size: 229 x 30"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/l/rkarhila/scratch/conda-envs/tensorflow3/lib/python3.5/site-packages/ipykernel/__main__.py:63: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/l/rkarhila/scratch/conda-envs/tensorflow3/lib/python3.5/site-packages/ipykernel/__main__.py:66: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------\n",
      "Array stats: start 8832 -> 69 length ?? -> 29 end 12544 -> 98\n",
      "      phone data size: 29 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 12544 -> 98 length ?? -> 10 end 13824 -> 108\n",
      "      phone data size: 10 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 13824 -> 108 length ?? -> 7 end 14720 -> 115\n",
      "      phone data size: 7 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 15872 -> 124 length ?? -> 73 end 25216 -> 197\n",
      "      phone data size: 73 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 25216 -> 197 length ?? -> 29 end 28928 -> 226\n",
      "      phone data size: 29 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 0 -> 0 length ?? -> 32 end 4096 -> 32\n",
      "      phone data size: 32 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 4096 -> 32 length ?? -> 36 end 8704 -> 68\n",
      "      phone data size: 36 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 8832 -> 69 length ?? -> 29 end 12544 -> 98\n",
      "      phone data size: 29 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 12544 -> 98 length ?? -> 10 end 13824 -> 108\n",
      "      phone data size: 10 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 13824 -> 108 length ?? -> 7 end 14720 -> 115\n",
      "      phone data size: 7 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 15872 -> 124 length ?? -> 73 end 25216 -> 197\n",
      "      phone data size: 73 x 30\n",
      "Data size: 229 x 30\n",
      "---------------------------\n",
      "Array stats: start 25216 -> 197 length ?? -> 29 end 28928 -> 226\n",
      "      phone data size: 29 x 30\n",
      "Data size: 229 x 30\n",
      "Item 2/878\n",
      "start feature extraction at 3840 (0.240000 s) and end at 29568 (1.848000 s) ==> 201 frames\n",
      "Utterance data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 0 -> 0 length ?? -> 27 end 3456 -> 27\n",
      "      phone data size: 27 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 3456 -> 27 length ?? -> 22 end 6272 -> 49\n",
      "      phone data size: 22 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 6656 -> 52 length ?? -> 21 end 9344 -> 73\n",
      "      phone data size: 21 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 9344 -> 73 length ?? -> 14 end 11136 -> 87\n",
      "      phone data size: 14 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 11136 -> 87 length ?? -> 28 end 14720 -> 115\n",
      "      phone data size: 28 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 15104 -> 118 length ?? -> 20 end 17664 -> 138\n",
      "      phone data size: 20 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 17664 -> 138 length ?? -> 63 end 25728 -> 201\n",
      "      phone data size: 63 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 0 -> 0 length ?? -> 27 end 3456 -> 27\n",
      "      phone data size: 27 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 3456 -> 27 length ?? -> 22 end 6272 -> 49\n",
      "      phone data size: 22 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 6656 -> 52 length ?? -> 21 end 9344 -> 73\n",
      "      phone data size: 21 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 9344 -> 73 length ?? -> 14 end 11136 -> 87\n",
      "      phone data size: 14 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 11136 -> 87 length ?? -> 28 end 14720 -> 115\n",
      "      phone data size: 28 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 15104 -> 118 length ?? -> 20 end 17664 -> 138\n",
      "      phone data size: 20 x 30\n",
      "Data size: 204 x 30\n",
      "---------------------------\n",
      "Array stats: start 17664 -> 138 length ?? -> 63 end 25728 -> 201\n",
      "      phone data size: 63 x 30\n",
      "Data size: 204 x 30\n",
      "Item 3/878\n",
      "start feature extraction at 2432 (0.152000 s) and end at 9984 (0.624000 s) ==> 59 frames\n",
      "Utterance data size: 62 x 30\n",
      "---------------------------\n",
      "Array stats: start 0 -> 0 length ?? -> 15 end 1920 -> 15\n",
      "      phone data size: 15 x 30\n",
      "Data size: 62 x 30\n",
      "---------------------------\n",
      "Array stats: start 1920 -> 15 length ?? -> 7 end 2816 -> 22\n",
      "      phone data size: 7 x 30\n",
      "Data size: 62 x 30\n",
      "---------------------------\n",
      "Array stats: start 3328 -> 26 length ?? -> 27 end 6784 -> 53\n",
      "      phone data size: 27 x 30\n",
      "Data size: 62 x 30\n",
      "---------------------------\n",
      "Array stats: start 6784 -> 53 length ?? -> 6 end 7552 -> 59\n",
      "      phone data size: 6 x 30\n",
      "Data size: 62 x 30\n",
      "---------------------------\n",
      "Array stats: start 0 -> 0 length ?? -> 15 end 1920 -> 15\n",
      "      phone data size: 15 x 30\n",
      "Data size: 62 x 30\n",
      "---------------------------\n",
      "Array stats: start 1920 -> 15 length ?? -> 7 end 2816 -> 22\n",
      "      phone data size: 7 x 30\n",
      "Data size: 62 x 30\n",
      "---------------------------\n",
      "Array stats: start 3328 -> 26 length ?? -> 27 end 6784 -> 53\n",
      "      phone data size: 27 x 30\n",
      "Data size: 62 x 30\n",
      "---------------------------\n",
      "Array stats: start 6784 -> 53 length ?? -> 6 end 7552 -> 59\n",
      "      phone data size: 6 x 30\n",
      "Data size: 62 x 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print (\"start!\")\n",
    "\n",
    "for collection in [collections[0]]:\n",
    "    triphonedata = []\n",
    "\n",
    "    recipefile = open( collection['recipe'] , 'r')\n",
    "    recipefilecounter = 0\n",
    "    too_long_counter = 0\n",
    "    all_trips_counter = 0\n",
    "\n",
    "    tmpfilecounter = 0\n",
    "\n",
    "    progress_interval = math.ceil(collection['numlines']/1000.0)\n",
    "\n",
    "    statistics_file=statistics_dir+\"/\"+corpus+\"-\"+collection['condition']+\"-\"+collection['name']+\".triphone-frame-counts\"\n",
    "    statistics_handle = open(statistics_file, 'w')\n",
    "\n",
    "    class_file=statistics_dir+\"/\"+corpus+\"-\"+collection['condition']+\"-\"+collection['name']+\".triphone-classes\"\n",
    "    class_handle= open(class_file, 'w')\n",
    "\n",
    "    phone_merge_file=statistics_dir+\"/\"+corpus+\"-\"+collection['condition']+\"-\"+collection['name']+\".phone-merge\"\n",
    "    phone_merge_handle = open(phone_merge_file, 'w')\n",
    "\n",
    "    quality_control_wavdir = os.path.join(pickle_dir, 'control-wav', collection['condition']+\"-\"+collection['name']+\"-classes_\"+classes_name)\n",
    "\n",
    "    mkdir(quality_control_wavdir)\n",
    "\n",
    "    quality_control_audio_files = {}\n",
    "\n",
    "    discard_counter=0\n",
    "    tooshortcount=0\n",
    "\n",
    "    for r in recipefile.readlines()[0:3]:\n",
    "        \n",
    "        recipefilecounter += 1\n",
    "        if debug:\n",
    "            print (\"Item %i/%i\" % (recipefilecounter, collection['numlines']) )\n",
    "\n",
    "        audiofile = re.sub('audio=', r'',  re.findall('audio=/[^ ]+', r)[0]).strip()\n",
    "        labelfile = re.sub(r'transcript=', r'', re.findall('transcript=/[^ ]+', r)[0]).strip()\n",
    "    \n",
    "        new_align = process_label(labelfile)\n",
    "        labelstring = get_labelstring( new_align )\n",
    "        \n",
    "        phone_merge_handle.write(\"%s\\t%s\\n\" % (labelfile, labelstring))\n",
    "\n",
    "        # OK, label file done.\n",
    "        # Now it's time to process the audio.\n",
    "        # We'll send to the feature extractor the bits of the file that \n",
    "        # match the speech segments.\n",
    "\n",
    "        if len(new_align) > 0:\n",
    "            triphonedata = get_features( audiofile, new_align)\n",
    "        \n",
    "            all_trips_counter += len( new_align )\n",
    "\n",
    "        if not debug:\n",
    "            if (recipefilecounter % int(progress_interval)) == 0:\n",
    "                sys.stderr.write(\"\\r%0.2f%s %s %s\" % (\n",
    "                        100.0*recipefilecounter/collection['numlines'], \n",
    "                        \"%\",\n",
    "                        collection['condition'], \n",
    "                        collection['name'] ))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "        if (recipefilecounter == collection['numlines']):\n",
    "            print (\"That's enough!\")\n",
    "            print (\"recipefilecounter %i  == collection['numlines'] %i\" % ( \n",
    "                    recipefilecounter, \n",
    "                    collection['numlines'] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle/save ##\n",
    "\n",
    "Next we'll save the audio data into variable length tensor flow thingies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if 1 == 0:    \n",
    "    for mono in sorted(triphonedata.keys()):\n",
    "    \n",
    "        grande_features =  np.zeros([0, max_num_frames, feature_dimension], dtype='float')\n",
    "        grande_classes = np.zeros([0, max_num_classes ], dtype='float')\n",
    "\n",
    "        for tripkey in sorted(triphonedata[mono].keys()):\n",
    "        \n",
    "            tripdata = triphonedata[mono][tripkey]\n",
    "            trip =  tripdata['triphone']\n",
    "\n",
    "            #print collection['name'][-1]\n",
    "            if trip in triphoneclasses.keys():\n",
    "                tripcl = triphoneclasses[trip]\n",
    "            else:\n",
    "                tripcl = len(triphoneclasses)\n",
    "                triphoneclasses[trip] = tripcl                \n",
    "                class_handle.write(\"%s\\t%i\\t%i\\n\" % (trip, tripcl, tripdata['counter']))\n",
    "                \n",
    "            grande_features = np.append(grande_features, tripdata['data'][0:tripdata['counter'],:,:], 0)\n",
    "\n",
    "            piccolo_classes =  np.zeros([ max_num_classes ], dtype='float')\n",
    "            piccolo_classes[tripcl] = 1\n",
    "            grande_classes = np.append(grande_classes, np.tile(piccolo_classes,(tripdata['counter'],1)),0)\n",
    "\n",
    "            \n",
    "\n",
    "        modeldir_unicode = mono\n",
    "\n",
    "        new_path=os.path.join(pickle_dir, collection['condition']+\"-\"+collection['name']+\"-classes_\"+classes_name)\n",
    "\n",
    "        picklefile = os.path.join(new_path,  collection['condition']+\"-\"+collection['name'] +\".\"+modeldir_unicode+\".pkl\")\n",
    "\n",
    "\n",
    "        mkdir(new_path)\n",
    "            \n",
    "        print (\"pickling %i items to %s\" % ( grande_features.shape[0], picklefile))\n",
    "                \n",
    "        outf = open(picklefile, 'wb')\n",
    "        \n",
    "        # Pickle the list using the highest protocol available.\n",
    "        cPickle.dump({'data': grande_features, 'classes': grande_classes}, outf, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load pickles and create sort of balanced sets:\n",
    "\n",
    "for collection in collections:\n",
    "\n",
    "    pickle_path=os.path.join(pickle_dir, collection['condition']+\"-\"+collection['name']+\"-classes_\"+classes_name)\n",
    "\n",
    "    classcount=0\n",
    "    \n",
    "    samplecounts=np.zeros([1000])\n",
    "\n",
    "    all_data_dict = {}\n",
    "\n",
    "\n",
    "    for picklefile in os.listdir(pickle_path):\n",
    "\n",
    "        #m = re.search(r'\\.([^.]+)\\.pkl$', picklefile.encode('utf-8').strip() )\n",
    "        m = re.search(r'\\.([^.]+)\\.pkl$', picklefile.strip() )\n",
    "        mono = m.group(1)\n",
    "\n",
    "        if mono:\n",
    "            #print \"Loading \"+mono\n",
    "            pickledata = cPickle.load(open(os.path.join(pickle_path,picklefile), 'r'))\n",
    "\n",
    "            if (featdim1 < 0):\n",
    "                featdim1 = pickledata['data'].shape[1]\n",
    "                featdim2 = pickledata['data'].shape[2]\n",
    "            \n",
    "            #all_data_dict[mono] = pickledata['data']\n",
    "            if mono not in classes.keys():\n",
    "                classes[mono] = classcount\n",
    "\n",
    "            print (\"Phone %s: %i samples\" % (mono, pickledata['data'].shape[0]))\n",
    "\n",
    "            samplecounts[classcount] = pickledata['data'].shape[0]\n",
    "\n",
    "            #grande_features =  np.zeros([0, max_num_frames, feature_dimension], dtype='float')\n",
    "            #grande_classes = np.zeros([0, max_num_classes ], dtype='float')\n",
    "\n",
    "\n",
    "            classcount+=1\n",
    "    \n",
    "    samplecounts = samplecounts[0:classcount]\n",
    "\n",
    "    print \"Samplecount %i   Classcount %i   Mean/median samples per class %0.2f / %0.1f    Min samples %i   Max samples %i\" % \\\n",
    "        (np.sum(samplecounts), \n",
    "         samplecounts.shape[0], \n",
    "         np.mean(samplecounts),\n",
    "         np.median(samplecounts),\n",
    "         np.min(samplecounts),\n",
    "         np.max(samplecounts));\n",
    "\n",
    "    print \"capping to min of avg/median\"\n",
    "\n",
    "    clip = math.ceil(min(np.median(samplecounts), np.mean(samplecounts) ))\n",
    "\n",
    "    samplecounts = np.clip(samplecounts, np.min(samplecounts), clip)\n",
    "    print \"Samplecount %i   Classcount %i   Mean/median samples per class %0.2f / %0.1f    Min samples %i   Max samples %i\" % \\\n",
    "        (np.sum(samplecounts), \n",
    "         samplecounts.shape[0], \n",
    "         np.mean(np.sum(samplecounts)/classcount),\n",
    "         np.median(samplecounts),\n",
    "         np.min(samplecounts),\n",
    "         np.max(samplecounts));\n",
    "\n",
    "    max_num_monoclasses=len(samplecounts)\n",
    "\n",
    "\n",
    "    grande_feature_array = np.zeros([np.sum(samplecounts), featdim1, featdim2])\n",
    "    grande_class_array = np.zeros([np.sum(samplecounts), max_num_monoclasses])\n",
    "\n",
    "    sample_counter = 0\n",
    "\n",
    "    for picklefile in os.listdir(pickle_path):\n",
    "\n",
    "        m = re.search(r'\\.([^.]+)\\.pkl$', picklefile.strip() )\n",
    "        mono = m.group(1)\n",
    "\n",
    "        if mono:\n",
    "\n",
    "            pickledata = cPickle.load(open(os.path.join(pickle_path,picklefile), 'r'))\n",
    "\n",
    "            trclass = classes[mono]\n",
    "\n",
    "            print \"Mono: %s Class: %i\" % (mono, trclass)\n",
    "\n",
    "            for i in np.random.permutation(np.arange(0, \n",
    "                                                     pickledata['data'].shape[0]))[0:min( clip ,\n",
    "                                                                                          pickledata['data'].shape[0])]:\n",
    "                \n",
    "                grande_feature_array[sample_counter,:,:]=pickledata['data'][i,:,:]\n",
    "                grande_class_array[sample_counter,trclass] = 1\n",
    "\n",
    "                sample_counter += 1\n",
    "\n",
    "    \n",
    "\n",
    "    if new_pickle_dir == \"-1\":\n",
    "        new_pickle_dir = os.path.join(pickle_dir, collection['condition']+\"-classes_\"+classes_name+\"_\"+str(sample_counter)+\"_pickled\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(new_pickle_dir)\n",
    "            \n",
    "        except OSError as exc:  # Python >2.5\n",
    "            if exc.errno == errno.EEXIST and os.path.isdir(new_pickle_dir):\n",
    "                pass\n",
    "            else:\n",
    "                raise   \n",
    "\n",
    "        classfile = os.path.join(new_pickle_dir,  collection['condition']+\".classes\")\n",
    "    \n",
    "        print \"Saving class map to %s\" % ( classfile);\n",
    "        class_handle = open(classfile, 'w')\n",
    "        for m in classes.keys():\n",
    "            class_handle.write(\"%i\\t%s\\n\" % (classes[m], m))\n",
    "        class_handle.close()\n",
    "\n",
    "        normalisation_file =  os.path.join(new_pickle_dir,  collection['condition']+\".mean_and_std.pkl\")\n",
    "\n",
    "\n",
    "        means = np.mean(np.vstack(grande_feature_array),0)\n",
    "        stds = np.std(np.vstack(grande_feature_array),0)\n",
    "        normoutf= open(normalisation_file, 'wb')\n",
    "\n",
    "\n",
    "        print \"pickling normalisation stats to %s\" % ( normalisation_file);\n",
    "\n",
    "        cPickle.dump({'mean': means, 'std': stds}, normoutf, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        means_set = True\n",
    "\n",
    "    shuf = np.random.permutation(np.arange(0,grande_feature_array.shape[0]))\n",
    "\n",
    "    a = ((grande_feature_array-means)/stds)[shuf]\n",
    "    b = grande_class_array[shuf]\n",
    "\n",
    "\n",
    "    targetpicklefile = os.path.join(new_pickle_dir,collection['condition']+\"-\"+collection['name']+\"-classes_\"+classes_name+\".pkl\" )\n",
    "\n",
    "    print \"pickling %i items to %s\" % ( grande_feature_array.shape[0], targetpicklefile);\n",
    "\n",
    "    outf = open(targetpicklefile, 'wb')\n",
    "    \n",
    "    # Pickle the list using the highest protocol available.\n",
    "    #cPickle.dump({'data': grande_feature_array, 'classes': grande_class_array}, outf, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    cPickle.dump({'data': a, 'classes': b}, outf, protocol=cPickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
